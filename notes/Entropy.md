# Entropy:
In information theory, the entropy of a random variable is the average level of "information", "surprise", or "uncertainty" inherent to the variable's possible outcomes. Given a discrete random variable $𝑋$, which takes  and is distributed according to $𝑝:𝑋→[0,1]$
$$\sum_i{P(x)*log(p(x))}$$
# Cross-entropy:
$$\sum_i{P(x)*log(q(x))}$$
* ## $p(x)$: true possibility
* ## $q(x)$: guessed possibility